{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c279ba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: googletrans==3.1.0a0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (3.1.0a0)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.1.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement collections (from versions: none)\n",
      "ERROR: No matching distribution found for collections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: contractions in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: autocorrect in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: unidecode in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: rake_nltk in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from rake_nltk) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake_nltk) (0.4.4)\n",
      "Requirement already satisfied: yake in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (0.4.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yake) (1.21.5)\n",
      "Requirement already satisfied: click>=6.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yake) (8.0.4)\n",
      "Requirement already satisfied: jellyfish in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yake) (0.11.2)\n",
      "Requirement already satisfied: segtok in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yake) (1.5.11)\n",
      "Requirement already satisfied: networkx in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yake) (2.7.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yake) (0.8.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from click>=6.0->yake) (0.4.4)\n",
      "Requirement already satisfied: regex in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from segtok->yake) (2022.3.15)\n",
      "Requirement already satisfied: gensim in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: yellowbrick in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (1.5)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yellowbrick) (1.7.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yellowbrick) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yellowbrick) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from yellowbrick) (1.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (2.2.0)\n",
      "Requirement already satisfied: imblearn in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\mohammed qadir\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install googletrans==3.1.0a0\n",
    "!pip install collections\n",
    "!pip install wordcloud\n",
    "!pip install contractions\n",
    "!pip install autocorrect\n",
    "!pip install unidecode\n",
    "!pip install rake_nltk\n",
    "!pip install yake\n",
    "!pip install gensim\n",
    "!pip install yellowbrick\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733aee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "# for language detection\n",
    "from langdetect import detect\n",
    "\n",
    "# for language translation\n",
    "from googletrans import Translator\n",
    "\n",
    "# to get counts and most common elements\n",
    "from collections import Counter\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to get wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# to fetch ngrams\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for data analysis and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to expand the text (don't to do not)\n",
    "import contractions\n",
    "\n",
    "# to auto correct the words\n",
    "from autocorrect import Speller\n",
    "\n",
    "# for handling accented characters (Á to A)\n",
    "from unidecode import unidecode\n",
    "\n",
    "# to get list of punctuations\n",
    "from string import punctuation\n",
    "\n",
    "# nltk libraries \n",
    "from nltk.tokenize import word_tokenize # to make tokens (sentense/words)\n",
    "from nltk.corpus import stopwords # to get list of stopwords\n",
    "from nltk.stem import WordNetLemmatizer # to find root words\n",
    "\n",
    "# for extracting the keywords\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# for making vectors of the documents\n",
    "from gensim.models import Word2Vec, keyedvectors\n",
    "\n",
    "# to see how the clusters are generated\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# scikit_learn libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# to balance the data\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21181bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"D:/Projects/EmailSpam/spam.csv\"\n",
    "df = pd.read_csv(path,encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4face3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renaming the columns\n",
    "df = df[[\"v1\",\"v2\"]]\n",
    "df.rename({\"v1\": \"target\",\"v2\":\"text\"},axis = 1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e122b7ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'id', 'so', 'af', 'it', 'ca', 'da', 'cy', 'sk', 'sl', 'fr',\n",
       "       'nl', 'tl', 'es', 'vi', 'no', 'de', 'sq', 'et', 'sv', 'pt', 'sw',\n",
       "       'fi', 'pl', 'hr', '', 'ro', 'tr', 'cs', 'lt'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the function to detect languages\n",
    "# If we have empty string so langdetect will give error. So, I am using try except.\n",
    "def lang_detect(data):\n",
    "    m = \"\"\n",
    "    try:\n",
    "        m = detect(data)\n",
    "    except:\n",
    "        pass\n",
    "    return m\n",
    "s = pd.Series(df[\"text\"].apply(lang_detect))\n",
    "s.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a4b84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar... Joking with u and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                                mail  \n",
       "0  Go until jurong point, crazy.. Available only ...  \n",
       "1                   Ok lar... Joking with u and i...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "3  U dun say so early hor... U c already then say...  \n",
       "4  Nah I don't think he goes to usf, he lives aro...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining the function for translation\n",
    "def lang_translate(data):\n",
    "    translor = Translator()\n",
    "    tr_text = translor.translate(data,dest=\"en\")\n",
    "    return tr_text.text\n",
    "df[\"mail\"] = df[\"text\"].apply(lang_translate)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88727a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we know translator takes high time to translate.\n",
    "# So I am exporting this file for further use.\n",
    "df.to_csv(\"translated_mail.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4149f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the text features as we have new features mail with translated mails\n",
    "df.drop(\"text\",axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d30990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  5572 non-null   object\n",
      " 1   mail    5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87bd696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "mail      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26ab239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = df.target.value_counts()\n",
    "plt.figure(figsize = (6,3))\n",
    "_.plot(kind = \"bar\",color = \"r\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.title(\"Category Wise Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd786c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(data, n_range):\n",
    "    tokens = data.split()\n",
    "    ngrms  = ngrams(tokens, n_range ) # provide zip file\n",
    "    ngrms_lst = []\n",
    "    for i in ngrms: #unzipping the file\n",
    "        ngrms_lst.append(\" \".join(i))\n",
    "    return ngrms_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f300a344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, until, jurong, point,, crazy.., Available...\n",
       "1             [Ok, lar..., Joking, with, u, and, i...]\n",
       "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
       "3    [U, dun, say, so, early, hor..., U, c, already...\n",
       "4    [Nah, I, don't, think, he, goes, to, usf,, he,...\n",
       "5    [FreeMsg, Hey, there, darling, it's, been, 3, ...\n",
       "6    [Even, my, brother, is, not, like, to, speak, ...\n",
       "7    [As, per, your, request, 'Melle, Melle, (Oru, ...\n",
       "8    [WINNER!!, As, a, valued, network, customer, y...\n",
       "9    [Had, your, mobile, 11, months, or, more?, U, ...\n",
       "Name: mail, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams = df.mail.apply(lambda x: get_ngrams(x,1))\n",
    "unigrams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f758e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 2147),\n",
       " ('you', 1635),\n",
       " ('I', 1473),\n",
       " ('a', 1343),\n",
       " ('the', 1208),\n",
       " ('and', 903),\n",
       " ('in', 808),\n",
       " ('is', 788),\n",
       " ('i', 742),\n",
       " ('u', 680)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "for i in unigrams:\n",
    "    lst.extend(i)\n",
    "Counter(lst).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b9384cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I'll call later\", 39),\n",
       " (\"Sorry, I'll call\", 38),\n",
       " ('. . .', 26),\n",
       " ('I want to', 24),\n",
       " ('have won a', 23),\n",
       " ('I miss you', 20),\n",
       " ('prize GUARANTEED. Call', 19),\n",
       " ('å£1000 cash or', 17),\n",
       " ('are trying to', 16),\n",
       " ('Account Statement for', 16)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheking most repeted 10 trigrams\n",
    "trigram = df.mail.apply(lambda x: get_ngrams(x,3))\n",
    "lst1 = []\n",
    "for i in trigram:\n",
    "    lst1.extend(i)\n",
    "Counter(lst1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1ff2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordcloud(data,column):\n",
    "    df_ = data[column].str.cat(sep = \" \")\n",
    "    text = \" \".join([i for i in df_.split()])\n",
    "    wcloud = WordCloud(width = 700, height = 500, background_color = \"lightgreen\").generate(text)\n",
    "    plt.figure(figsize = (10,6))\n",
    "    plt.imshow(wcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30fc1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to extract keywords\n",
    "def get_keypharses(data):\n",
    "    key_ex = yake.KeywordExtractor()\n",
    "    keyword = key_ex.extract_keywords(data) # give (kewword, score) in a tuple\n",
    "    kw_lst = []\n",
    "    for i in keyword:\n",
    "        kw_lst.append(i[0]) # taking only keyword\n",
    "    return kw_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2051661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [crazy., jurong point, point, jurong, buffet, ...\n",
       "1                                        [lar, Joking]\n",
       "2    [Cup final tkts, win FA Cup, Cup final, final ...\n",
       "3                         [early hor, hor, dun, early]\n",
       "4                                    [Nah, usf, lives]\n",
       "5    [FreeMsg Hey, Hey there darling, word back, He...\n",
       "6    [aids patent, brother, speak, patent, treat, a...\n",
       "7    [Oru Minnaminunginte Nurungu, Minnaminunginte ...\n",
       "8    [WINNER, prize reward, claim, Valid, prize, re...\n",
       "9    [Update, Mobile Update, months, Free, mobile, ...\n",
       "Name: mail, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw = df.mail.apply(get_keypharses)\n",
    "kw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b7d3e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('selected to receive', 13),\n",
       " ('attempt to contact', 13),\n",
       " ('pick the phone', 12),\n",
       " ('send a message', 12),\n",
       " ('customer service representative', 10),\n",
       " ('WON a guaranteed', 10),\n",
       " ('call our customer', 10),\n",
       " ('draw txt MUSIC', 9),\n",
       " ('chance to win', 9),\n",
       " ('anytime any network', 9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstk = []\n",
    "for i in kw:\n",
    "    lstk.extend(i)\n",
    "Counter([i for i in lstk if len(i.split()) > 2]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8232b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "def remove_spaces(data):\n",
    "    clean_text = data.replace(\"\\\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\\\\",\" \")\n",
    "    return clean_text\n",
    "\n",
    "# defining the function for expanding text (can't to can not)\n",
    "def expand_text(data):\n",
    "    ex_text = contractions.fix(data)\n",
    "    return ex_text\n",
    "\n",
    "# defining the function for handling accented characters (Á to A)\n",
    "def handling_accented(data):\n",
    "    fix_text = unidecode(data)\n",
    "    return fix_text\n",
    "\n",
    "# defining the function for removing stopwords\n",
    "stopword = stopwords.words(\"english\") # gives a list of stopwords\n",
    "stopword.remove(\"no\") # removing neccessary stop words from list\n",
    "stopword.remove(\"not\")\n",
    "stopword.remove(\"nor\")\n",
    "\n",
    "def clean_text(data):\n",
    "    token = word_tokenize(data)\n",
    "    clean_text = [i.lower() for i in token if (i not in punctuation) \n",
    "                  and (i.lower() not in stopword) and (i.isalpha()) and (len(i) > 2)]\n",
    "    return clean_text\n",
    "\n",
    "# defining the function for auto correction \n",
    "def auto_correct(data):\n",
    "    spell = Speller(lang = \"en\")\n",
    "    text = spell(data)\n",
    "    return text\n",
    "\n",
    "# defining the function for getting root words\n",
    "def lemmatization(data):\n",
    "    lem = WordNetLemmatizer()\n",
    "    lst1 = []\n",
    "    for i in data:\n",
    "        lem_words = lem.lemmatize(i)\n",
    "        lst1.append(lem_words)\n",
    "    return \" \".join(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfbacb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93fb257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Mohammed\n",
      "[nltk_data]     Qadir\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f925e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_mails\"] = df[\"mail\"].apply(remove_spaces)\n",
    "df[\"clean_mails\"] = df[\"clean_mails\"].apply(expand_text)\n",
    "df[\"clean_mails\"] = df[\"clean_mails\"].apply(handling_accented)\n",
    "df[\"clean_mails\"] = df[\"clean_mails\"].apply(clean_text)\n",
    "df[\"clean_mails\"] = df[\"clean_mails\"].apply(lambda x: auto_correct(x) if isinstance(x, str) or isinstance(x, bytes) else x)\n",
    "df[\"clean_mails\"] = df[\"clean_mails\"].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "896ba7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>clean_mails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>jurong point crazy available bugis great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>lar joking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                        clean_mails\n",
       "0    ham  jurong point crazy available bugis great world...\n",
       "1    ham                                         lar joking"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"mail\",axis = 1, inplace = True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8caa4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.replace({\"spam\": 0, \"ham\": 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11586b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data to avoid data leakage\n",
    "x = df[\"clean_mails\"]\n",
    "y = df[\"target\"]\n",
    "x_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size=0.25,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d6ad1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vectors of the mails\n",
    "cnv = CountVectorizer(max_df=0.95,max_features=1000,min_df = 10, stop_words=\"english\",lowercase=True)\n",
    "x_train = cnv.fit_transform(x_tr).A\n",
    "x_test  = cnv.transform(x_te).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e80cff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using SMOTE to avoid data duplicacy\n",
    "st = SMOTE(sampling_strategy=0.75,random_state=42)\n",
    "sm_x_train,sm_y_train = st.fit_resample(x_train,y_tr)\n",
    "sm_x_test,sm_y_test   = st.fit_resample(x_test,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b45af83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first we have\n",
      "1    3619\n",
      "0     560\n",
      "Name: target, dtype: int64\n",
      "After balancing we have\n",
      "1    3619\n",
      "0    2714\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"first we have\\n{y_tr.value_counts()}\\nAfter balancing we have\\n{sm_y_train.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0abb4109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Multinomial Naive Bayes model:\n",
      "At Training:  0.8863098057792516\n",
      "At Testing:  0.8232227488151659\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "The accuracy of the Gaussian Naive Bayes model:\n",
      "At Training:  0.8120953734407074\n",
      "At Testing:  0.7829383886255924\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "The accuracy of the XG Boost model:\n",
      "At Training:  0.8926259276804043\n",
      "At Testing:  0.8061611374407583\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# selecting the model\n",
    "models = [(\"Multinomial Naive Bayes\", MultinomialNB()),\n",
    "          (\"Gaussian Naive Bayes\", GaussianNB()),\n",
    "          (\"XG Boost\", XGBClassifier())]\n",
    "for name,model in models:\n",
    "    mod = model\n",
    "    mod.fit(sm_x_train,sm_y_train)\n",
    "    pred = mod.predict(sm_x_train)\n",
    "    acc = accuracy_score(sm_y_train,pred)\n",
    "    pred1 = mod.predict(sm_x_test)\n",
    "    acc1 = accuracy_score(sm_y_test,pred1)\n",
    "    print(f\"The accuracy of the {name} model:\\nAt Training: \",acc)\n",
    "    print(\"At Testing: \",acc1)\n",
    "    print(\"+\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a506b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END alpha=10, fit_prior=False;, score=(train=0.909, test=0.895) total time=   0.0s\n",
      "[CV 2/3] END alpha=10, fit_prior=False;, score=(train=0.914, test=0.910) total time=   0.0s\n",
      "[CV 3/3] END alpha=10, fit_prior=False;, score=(train=0.908, test=0.909) total time=   0.0s\n",
      "[CV 1/3] END alpha=0.001, fit_prior=False;, score=(train=0.928, test=0.892) total time=   0.0s\n",
      "[CV 2/3] END alpha=0.001, fit_prior=False;, score=(train=0.923, test=0.922) total time=   0.0s\n",
      "[CV 3/3] END alpha=0.001, fit_prior=False;, score=(train=0.922, test=0.923) total time=   0.0s\n",
      "[CV 1/3] END alpha=0.1, fit_prior=False;, score=(train=0.928, test=0.899) total time=   0.0s\n",
      "[CV 2/3] END alpha=0.1, fit_prior=False;, score=(train=0.922, test=0.923) total time=   0.0s\n",
      "[CV 3/3] END alpha=0.1, fit_prior=False;, score=(train=0.922, test=0.923) total time=   0.0s\n",
      "[CV 1/3] END alpha=100, fit_prior=False;, score=(train=0.896, test=0.881) total time=   0.0s\n",
      "[CV 2/3] END alpha=100, fit_prior=False;, score=(train=0.896, test=0.892) total time=   0.0s\n",
      "[CV 3/3] END alpha=100, fit_prior=False;, score=(train=0.890, test=0.888) total time=   0.0s\n",
      "[CV 1/3] END alpha=1000, fit_prior=False;, score=(train=0.891, test=0.880) total time=   0.0s\n",
      "[CV 2/3] END alpha=1000, fit_prior=False;, score=(train=0.885, test=0.876) total time=   0.0s\n",
      "[CV 3/3] END alpha=1000, fit_prior=False;, score=(train=0.884, test=0.880) total time=   0.0s\n",
      "[CV 1/3] END alpha=0.0001, fit_prior=True;, score=(train=0.872, test=0.893) total time=   0.0s\n",
      "[CV 2/3] END alpha=0.0001, fit_prior=True;, score=(train=0.896, test=0.868) total time=   0.0s\n",
      "[CV 3/3] END alpha=0.0001, fit_prior=True;, score=(train=0.900, test=0.865) total time=   0.0s\n",
      "[CV 1/3] END alpha=1e-05, fit_prior=True;, score=(train=0.872, test=0.892) total time=   0.0s\n",
      "[CV 2/3] END alpha=1e-05, fit_prior=True;, score=(train=0.896, test=0.868) total time=   0.0s\n",
      "[CV 3/3] END alpha=1e-05, fit_prior=True;, score=(train=0.900, test=0.865) total time=   0.0s\n",
      "[CV 1/3] END alpha=0.0001, fit_prior=False;, score=(train=0.929, test=0.889) total time=   0.0s\n",
      "[CV 2/3] END alpha=0.0001, fit_prior=False;, score=(train=0.923, test=0.922) total time=   0.0s\n",
      "[CV 3/3] END alpha=0.0001, fit_prior=False;, score=(train=0.922, test=0.923) total time=   0.0s\n",
      "[CV 1/3] END alpha=1e-05, fit_prior=False;, score=(train=0.929, test=0.887) total time=   0.0s\n",
      "[CV 2/3] END alpha=1e-05, fit_prior=False;, score=(train=0.923, test=0.922) total time=   0.0s\n",
      "[CV 3/3] END alpha=1e-05, fit_prior=False;, score=(train=0.922, test=0.923) total time=   0.0s\n",
      "[CV 1/3] END alpha=10, fit_prior=True;, score=(train=0.856, test=0.907) total time=   0.0s\n",
      "[CV 2/3] END alpha=10, fit_prior=True;, score=(train=0.891, test=0.858) total time=   0.0s\n",
      "[CV 3/3] END alpha=10, fit_prior=True;, score=(train=0.886, test=0.855) total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_prior': False, 'alpha': 0.1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "estimator = MultinomialNB()\n",
    "params_NB = {'alpha': [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000],\n",
    "            \"fit_prior\":[True,False]} \n",
    "rnd = RandomizedSearchCV(estimator=estimator, param_distributions=params_NB,\n",
    "                         cv= 3, verbose=3, scoring='accuracy',return_train_score=True) \n",
    "rnd.fit(sm_x_train,sm_y_train)\n",
    "rnd.best_params_\n",
    "#increase the CV to 7 when training on a high powered gpu else it will take high time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67acb215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(alpha=1, fit_prior=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=1, fit_prior=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(alpha=1, fit_prior=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "mlnb = MultinomialNB(fit_prior = False, alpha = 1)\n",
    "mlnb.fit(sm_x_train,sm_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcdab02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is:  0.9205747670930049\n",
      "\n",
      "\n",
      "The classifiaction report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      2714\n",
      "           1       0.98      0.88      0.93      3619\n",
      "\n",
      "    accuracy                           0.92      6333\n",
      "   macro avg       0.92      0.93      0.92      6333\n",
      "weighted avg       0.93      0.92      0.92      6333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = mlnb.predict(sm_x_train)\n",
    "acc = accuracy_score(sm_y_train,pred)\n",
    "clf = classification_report(sm_y_train,pred)\n",
    "print(\"Training accuracy is: \",acc)\n",
    "print(\"\\n\")\n",
    "print(\"The classifiaction report:\\n\",clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bee08a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  0.9\n",
      "\n",
      "\n",
      "The classifiaction report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89       904\n",
      "           1       0.95      0.87      0.91      1206\n",
      "\n",
      "    accuracy                           0.90      2110\n",
      "   macro avg       0.90      0.91      0.90      2110\n",
      "weighted avg       0.91      0.90      0.90      2110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = mlnb.predict(sm_x_test)\n",
    "acc1 = accuracy_score(sm_y_test,pred1)\n",
    "clf1 = classification_report(sm_y_test,pred1)\n",
    "print(\"Testing accuracy is: \",acc1)\n",
    "print(\"\\n\")\n",
    "print(\"The classifiaction report:\\n\",clf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b07c0f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mail is::  ham\n"
     ]
    }
   ],
   "source": [
    "# testing the model whether it works for userinput.\n",
    "dics = {1: \"ham\" , 0:\"spam\" }\n",
    "#user input\n",
    "text = \"Nah I don't think he goes to usf, he lives around here though\"\n",
    "\n",
    "# first we create a vector of this review\n",
    "vector1 = cnv.transform([text]).A\n",
    "\n",
    "# getting prediction\n",
    "prediction = mlnb.predict(vector1)\n",
    "\n",
    "# printing the results\n",
    "print(\"The mail is:: \",dics[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d9a9f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mail is::  spam\n"
     ]
    }
   ],
   "source": [
    "# testing the model by giving spam\n",
    "#user input\n",
    "texts = \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\"\n",
    "\n",
    "# first we create a vector of this review\n",
    "vector1 = cnv.transform([texts]).A\n",
    "\n",
    "# getting prediction\n",
    "prediction = mlnb.predict(vector1)\n",
    "\n",
    "# printing the results\n",
    "print(\"The mail is:: \" ,dics[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "936541b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"mlnb_model.pkl\",\"wb\") as f:\n",
    "    pickle.dump(mlnb,f)\n",
    "with open(\"countvectorizer.pkl\",\"wb\") as f:\n",
    "    pickle.dump(cnv,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5efe932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
